# [01] Python深度学习笔记 - 关于keras、numpy的基础操作

[TOC]

## 1. 关于数据

### 数据表示

在深度学习中数据用张量存储，张量是数据的容器。

仅包含一个数字的张量叫做**标量**（零维张量，0D张量）。

数字输出的张量叫做**向量**，一维张量只有一个轴。

向量组成的数组称为**矩阵**（matrix）。

多个矩阵合成新的数组，成为3D张量。

以此类推。



张量有如下关键属性：

- 轴的个数（阶）
- 形状：表示张量沿每个轴的维度大小
- 数据类型 dtype。（Numpy中不存在字符串张量）



### 一些函数或者属性：

轴的个数： `x.ndim`

形状：`x.shape`

数据类型：`x.dtype`



matplotlib负责处理图像，显示图片：

```
import matplotlib.pyplot as plt
```

处理图片、选择其格式：`plt.imshow(image, cmap=)`

cmap：colormaps

- `plt.cm`是matplotlib中设置的色彩映射函数
    - 有多种，mnist中是使用`plt.cm.binary`



### 张量切片：

**张量切片：** 选择张量的特定元素，与python切片原理基本一致。

比如一个三维张量，（60000， 28， 28）

选择其中的一部分：

`tensor[:, 7:-7, 7:-7]`



### 数据批量

深度学习中的第一个轴，都是**样本轴**。一般处理数据时，不会同时处理整个数据集，会将它拆成小批量的数据。



## 2. 张量运算

神经网络中的所有变换都可以简化为数值数据张量上的一些运算。

一个Dense层可以视为： 

```
output = relu(dot(W, input) + b)
```

其中，dot为点积运算，（+）运算和最后的relu运算，`relu(x) = max(x, 0)`

### 逐元素运算

`relu`和`(+)`都是逐元素运算，非常适合大规模并行实现

> BLAS 是低层次的、高度并行的、高效的张量操作程序，通常用 Fortran 或C语言来实现。 
> 因此，在 Numpy 中可以直接进行下列逐元素运算，速度非常快。import numpy as np 
> z = x + y  逐元素的相加 
> z = np.maximum(z, 0.) 逐元素的 relu



### 广播 Broadcast

两个不同形状的向量之间操作的话，较小的张量会被**广播**，以匹配较大张量的形状。

广播的过程：

（1）向较小的张量添加轴，使其 `ndim` 与较大的张量相同。

（2）将较小的张量沿着新轴重复，使其形状和较大的张量相同。

eg：

```
x(32, 10) y(10,)
```
首先，给 `y` 添加一个轴，然后将 `y` 沿着新轴重复32次，得到新的形状 `y(32, 10)`。
但是在实际的实现过程中并不会创建新的 `2D` 张量，因为会影响效率，这个行为并没有发生在内存中。



如果一个张量的形状是 `(a,b, ..., n, n+1, ..., m)`，另一个张量的形状是 `(n, n+1, ..., m)`，那么可以利用广播对它们进行逐元素运算，广播操作会自动应用于从a到n+1的轴。



### 张量点积

`z = np.dot(x, y)`

注意点：

对于两个矩阵 `x` 和 `y`，当且仅当 `x.shape[1] == y.shape[0]`时，才可以进行点积。其实就是矩阵乘法的要求。

更一般的：

```
(a, b, c, d) dot (d, ) = (a, b, c)
(a, b, c, d) dot (d, e) = (a, b, c, e)
```

### 张量变形

`reshape()`

注意点：

总个数不变

```
x = np.array([0.1, 1, 3, 4])
x = x.reshape((2,2))
```

**转置：**行列互换

`np.transpose(x)`



### 3. 基于梯度的优化

在`output = relu(dot(W, input)+b)` 中，`W` 和 `b` 被称为该层的权重(weight)或者可训练参数。

一开始，这些权重矩阵取较小的随机值，这一步叫作随机初始化（random initialization）。下一步则是根据反馈信号逐渐调节这些权重。

一个神经网络的训练循环：

